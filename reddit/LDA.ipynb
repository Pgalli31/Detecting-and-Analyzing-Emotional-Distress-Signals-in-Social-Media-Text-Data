{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf825ace-4a19-4403-b8c9-4de73ca7a71e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pgalli/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package wordnet to /Users/pgalli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pgalli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_md\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37501421-05d8-4e6d-9b0e-bf089ea86766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('reddit_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276e9c94-a569-4e54-84e3-8ae24e91d042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load(disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ']):\n",
    "    '''This function converts terms to their base forms.'''\n",
    "    output = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(str(sent))\n",
    "        output.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return output\n",
    "\n",
    "\n",
    "text_list = data['clean_post'].tolist()\n",
    "tokenized_text = lemmatization(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d85fea-1a38-41fa-a649-dc47e2d336c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "if len(dictionary) > 0:\n",
    "    doc_term_matrix = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "else:\n",
    "    doc_term_matrix = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5d9e0-e454-4d96-8929-5100bf4366f3",
   "metadata": {},
   "source": [
    "## 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a278dfc7-b0aa-46cd-b9f0-3836bddeca6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.074*\"life\" + 0.052*\"people\" + 0.032*\"fucking\" + 0.021*\"world\" + 0.020*\"shit\" + 0.020*\"fuck\" + 0.014*\"thing\" + 0.012*\"point\" + 0.012*\"way\" + 0.011*\"person\"'), (1, '0.104*\"job\" + 0.098*\"work\" + 0.016*\"hour\" + 0.015*\"money\" + 0.014*\"home\" + 0.012*\"new\" + 0.011*\"motivation\" + 0.010*\"energy\" + 0.009*\"day\" + 0.007*\"task\"'), (2, '0.030*\"year\" + 0.026*\"time\" + 0.023*\"friend\" + 0.021*\"life\" + 0.015*\"day\" + 0.015*\"thing\" + 0.015*\"family\" + 0.014*\"good\" + 0.010*\"month\" + 0.010*\"bad\"'), (3, '0.092*\"pain\" + 0.040*\"pill\" + 0.026*\"goodbye\" + 0.016*\"painless\" + 0.016*\"emptiness\" + 0.014*\"rope\" + 0.012*\"unbearable\" + 0.011*\"dose\" + 0.011*\"prescription\" + 0.009*\"body\"'), (4, '0.047*\"people\" + 0.039*\"thing\" + 0.030*\"friend\" + 0.024*\"time\" + 0.019*\"depression\" + 0.017*\"happy\" + 0.017*\"feeling\" + 0.016*\"depressed\" + 0.014*\"bad\" + 0.014*\"good\"'), (5, '0.075*\"money\" + 0.040*\"food\" + 0.035*\"car\" + 0.028*\"bill\" + 0.025*\"tooth\" + 0.023*\"homeless\" + 0.021*\"pay\" + 0.013*\"rent\" + 0.012*\"dollar\" + 0.010*\"distraction\"'), (6, '0.052*\"day\" + 0.038*\"life\" + 0.031*\"tired\" + 0.025*\"time\" + 0.022*\"pain\" + 0.015*\"bad\" + 0.014*\"end\" + 0.014*\"thing\" + 0.014*\"sleep\" + 0.013*\"well\"'), (7, '0.097*\"school\" + 0.037*\"college\" + 0.028*\"high\" + 0.026*\"parent\" + 0.023*\"year\" + 0.021*\"grade\" + 0.020*\"class\" + 0.013*\"study\" + 0.013*\"university\" + 0.012*\"summer\"'), (8, '0.093*\"suicidal\" + 0.093*\"thought\" + 0.073*\"suicide\" + 0.027*\"self\" + 0.020*\"gun\" + 0.016*\"harm\" + 0.014*\"attempt\" + 0.014*\"plan\" + 0.013*\"method\" + 0.013*\"boring\"'), (9, '0.060*\"depression\" + 0.032*\"mental\" + 0.029*\"anxiety\" + 0.027*\"help\" + 0.024*\"therapy\" + 0.023*\"health\" + 0.020*\"therapist\" + 0.019*\"doctor\" + 0.018*\"medication\" + 0.016*\"month\"')]\n"
     ]
    }
   ],
   "source": [
    "if doc_term_matrix:\n",
    "    LDA = gensim.models.ldamodel.LdaModel\n",
    "    lda_model = LDA(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=dictionary,\n",
    "        num_topics=10,\n",
    "        random_state=100,\n",
    "        chunksize=1000,\n",
    "        passes=50,\n",
    "        iterations=100\n",
    "    )\n",
    "    print(lda_model.print_topics())\n",
    "else:\n",
    "    print(\"Document term matrix is empty, cannot build LDA model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8068e518-684d-404c-af4f-68ece70ef4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: -7.829607360960524\n",
      "Coherence: 0.451284553832768\n"
     ]
    }
   ],
   "source": [
    "total_docs = len(doc_term_matrix)\n",
    "if total_docs > 0:\n",
    "    print('\\nPerplexity:', lda_model.log_perplexity(\n",
    "        doc_term_matrix, total_docs=total_docs))\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=tokenized_text,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence:', coherence_lda)\n",
    "else:\n",
    "    print(\"No documents to evaluate coherence or perplexity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d6ecf2-cd7f-4225-a03a-c678338d023a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if total_docs > 0:\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis_data = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "    vis_data\n",
    "    pyLDAvis.save_html(vis_data, '10_reddit_lda_visualization.html')\n",
    "else:\n",
    "    print(\"No documents for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552f6de-5c63-43de-afa0-555b4c35edb0",
   "metadata": {},
   "source": [
    "## 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20dc7477-30e2-4425-b557-1318026e004f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.182*\"people\" + 0.065*\"world\" + 0.043*\"human\" + 0.028*\"person\" + 0.024*\"other\" + 0.023*\"society\" + 0.019*\"message\" + 0.019*\"selfish\" + 0.016*\"shitty\" + 0.015*\"existence\"'), (1, '0.353*\"suicide\" + 0.048*\"note\" + 0.024*\"plan\" + 0.019*\"hotline\" + 0.017*\"temporary\" + 0.014*\"mechanism\" + 0.013*\"question\" + 0.012*\"track\" + 0.012*\"police\" + 0.012*\"inevitable\"'), (2, '0.037*\"friend\" + 0.031*\"time\" + 0.030*\"thing\" + 0.027*\"people\" + 0.019*\"good\" + 0.019*\"bad\" + 0.014*\"year\" + 0.013*\"life\" + 0.012*\"way\" + 0.012*\"person\"'), (3, '0.082*\"depression\" + 0.067*\"mental\" + 0.044*\"help\" + 0.043*\"health\" + 0.038*\"anxiety\" + 0.034*\"therapy\" + 0.027*\"doctor\" + 0.026*\"medication\" + 0.026*\"therapist\" + 0.025*\"bad\"'), (4, '0.036*\"ugly\" + 0.035*\"loneliness\" + 0.031*\"weight\" + 0.029*\"amp\" + 0.025*\"hair\" + 0.023*\"personality\" + 0.022*\"body\" + 0.021*\"medium\" + 0.020*\"fat\" + 0.020*\"attractive\"'), (5, '0.109*\"tomorrow\" + 0.109*\"ready\" + 0.063*\"episode\" + 0.049*\"depressive\" + 0.047*\"tooth\" + 0.033*\"shower\" + 0.020*\"bunch\" + 0.019*\"crash\" + 0.019*\"pregnant\" + 0.019*\"weekend\"'), (6, '0.318*\"tired\" + 0.037*\"sick\" + 0.035*\"sad\" + 0.034*\"feeling\" + 0.027*\"empty\" + 0.018*\"exhausted\" + 0.017*\"escape\" + 0.015*\"light\" + 0.012*\"push\" + 0.011*\"darkness\"'), (7, '0.120*\"fucking\" + 0.085*\"shit\" + 0.076*\"fuck\" + 0.027*\"hate\" + 0.024*\"stupid\" + 0.018*\"piece\" + 0.015*\"useless\" + 0.013*\"worthless\" + 0.013*\"failure\" + 0.012*\"sorry\"'), (8, '0.266*\"self\" + 0.071*\"tonight\" + 0.060*\"harm\" + 0.031*\"wrist\" + 0.020*\"knife\" + 0.020*\"head\" + 0.019*\"brain\" + 0.014*\"numbness\" + 0.013*\"voice\" + 0.011*\"tw\"'), (9, '0.048*\"heart\" + 0.028*\"body\" + 0.024*\"memory\" + 0.023*\"movie\" + 0.021*\"word\" + 0.020*\"blood\" + 0.019*\"arm\" + 0.018*\"mind\" + 0.017*\"emotion\" + 0.016*\"reality\"'), (10, '0.035*\"mum\" + 0.034*\"attempt\" + 0.023*\"hospital\" + 0.014*\"helpful\" + 0.013*\"floor\" + 0.013*\"cancer\" + 0.013*\"bpd\" + 0.012*\"overdose\" + 0.012*\"grief\" + 0.011*\"diagnosis\"'), (11, '0.189*\"pain\" + 0.064*\"way\" + 0.055*\"pill\" + 0.038*\"painful\" + 0.035*\"gun\" + 0.031*\"kill\" + 0.028*\"suffering\" + 0.027*\"death\" + 0.024*\"method\" + 0.020*\"end\"'), (12, '0.289*\"thought\" + 0.261*\"suicidal\" + 0.029*\"antidepressant\" + 0.020*\"intrusive\" + 0.018*\"rock\" + 0.014*\"bottom\" + 0.009*\"height\" + 0.009*\"torture\" + 0.009*\"experience\" + 0.007*\"toi\"'), (13, '0.105*\"life\" + 0.030*\"thing\" + 0.022*\"day\" + 0.021*\"time\" + 0.020*\"happy\" + 0.020*\"good\" + 0.018*\"people\" + 0.017*\"year\" + 0.015*\"well\" + 0.015*\"way\"'), (14, '0.079*\"mom\" + 0.065*\"parent\" + 0.048*\"family\" + 0.043*\"dad\" + 0.041*\"mother\" + 0.030*\"old\" + 0.029*\"brother\" + 0.028*\"year\" + 0.027*\"kid\" + 0.026*\"sister\"'), (15, '0.083*\"day\" + 0.037*\"time\" + 0.029*\"week\" + 0.025*\"night\" + 0.024*\"bed\" + 0.024*\"last\" + 0.023*\"sleep\" + 0.023*\"hour\" + 0.021*\"work\" + 0.020*\"month\"'), (16, '0.248*\"scared\" + 0.145*\"afraid\" + 0.028*\"jump\" + 0.027*\"bridge\" + 0.025*\"miss\" + 0.022*\"building\" + 0.022*\"white\" + 0.019*\"regret\" + 0.015*\"immediate\" + 0.013*\"pit\"'), (17, '0.112*\"man\" + 0.102*\"woman\" + 0.079*\"dog\" + 0.038*\"insurance\" + 0.021*\"trash\" + 0.020*\"rich\" + 0.018*\"office\" + 0.013*\"humanity\" + 0.011*\"animal\" + 0.011*\"basic\"'), (18, '0.117*\"game\" + 0.085*\"video\" + 0.055*\"music\" + 0.039*\"talk\" + 0.035*\"boring\" + 0.026*\"bottle\" + 0.023*\"bored\" + 0.023*\"interesting\" + 0.021*\"laugh\" + 0.013*\"anti\"'), (19, '0.049*\"job\" + 0.048*\"year\" + 0.032*\"school\" + 0.028*\"work\" + 0.027*\"life\" + 0.017*\"time\" + 0.016*\"money\" + 0.014*\"parent\" + 0.013*\"high\" + 0.012*\"college\"')]\n"
     ]
    }
   ],
   "source": [
    "if doc_term_matrix:\n",
    "    LDA = gensim.models.ldamodel.LdaModel\n",
    "    lda_model = LDA(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=dictionary,\n",
    "        num_topics=20,\n",
    "        random_state=100,\n",
    "        chunksize=1000,\n",
    "        passes=50,\n",
    "        iterations=100\n",
    "    )\n",
    "    print(lda_model.print_topics())\n",
    "else:\n",
    "    print(\"Document term matrix is empty, cannot build LDA model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa530dd-ec9b-4d63-87e3-a86fc65bb8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: -9.143902610364657\n",
      "Coherence: 0.40729784207988373\n"
     ]
    }
   ],
   "source": [
    "total_docs = len(doc_term_matrix)\n",
    "if total_docs > 0:\n",
    "    print('\\nPerplexity:', lda_model.log_perplexity(\n",
    "        doc_term_matrix, total_docs=total_docs))\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=tokenized_text,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence:', coherence_lda)\n",
    "else:\n",
    "    print(\"No documents to evaluate coherence or perplexity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b1a587-37f4-4bef-894b-b2df9b031a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if total_docs > 0:\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis_data = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "    vis_data\n",
    "    pyLDAvis.save_html(vis_data, '20_reddit_lda_visualization.html')\n",
    "else:\n",
    "    print(\"No documents for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf87fb-da2b-4f11-80df-2db91414de66",
   "metadata": {},
   "source": [
    "## 30 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f9be805-2773-469a-a328-d3c8083f548d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, '0.097*\"dog\" + 0.047*\"insurance\" + 0.045*\"nightmare\" + 0.044*\"bill\" + 0.041*\"medical\" + 0.037*\"pay\" + 0.032*\"debt\" + 0.030*\"symptom\" + 0.027*\"ok\" + 0.027*\"function\"'), (2, '0.044*\"guy\" + 0.042*\"girl\" + 0.032*\"woman\" + 0.030*\"man\" + 0.020*\"sex\" + 0.016*\"date\" + 0.016*\"relationship\" + 0.014*\"love\" + 0.014*\"drunk\" + 0.014*\"old\"'), (6, '0.042*\"exhausted\" + 0.041*\"deep\" + 0.036*\"hole\" + 0.033*\"bipolar\" + 0.028*\"bottle\" + 0.025*\"light\" + 0.025*\"black\" + 0.023*\"relief\" + 0.020*\"dark\" + 0.019*\"bpd\"'), (12, '0.166*\"normal\" + 0.097*\"attack\" + 0.089*\"panic\" + 0.038*\"rock\" + 0.035*\"urge\" + 0.033*\"weird\" + 0.033*\"bottom\" + 0.028*\"overwhelmed\" + 0.027*\"song\" + 0.022*\"torture\"'), (16, '0.190*\"kid\" + 0.075*\"child\" + 0.070*\"wife\" + 0.023*\"emptiness\" + 0.022*\"grandma\" + 0.021*\"service\" + 0.021*\"married\" + 0.019*\"fire\" + 0.017*\"rejection\" + 0.015*\"spot\"'), (24, '0.132*\"pain\" + 0.049*\"way\" + 0.049*\"death\" + 0.045*\"suicide\" + 0.044*\"alive\" + 0.040*\"dead\" + 0.037*\"end\" + 0.036*\"life\" + 0.033*\"kill\" + 0.032*\"wish\"'), (14, '0.123*\"mental\" + 0.080*\"health\" + 0.059*\"mother\" + 0.031*\"issue\" + 0.029*\"family\" + 0.026*\"illness\" + 0.025*\"abusive\" + 0.024*\"father\" + 0.021*\"hospital\" + 0.020*\"child\"'), (26, '0.099*\"life\" + 0.043*\"year\" + 0.030*\"thing\" + 0.025*\"time\" + 0.023*\"good\" + 0.020*\"people\" + 0.016*\"family\" + 0.016*\"friend\" + 0.016*\"bad\" + 0.014*\"day\"'), (19, '0.097*\"school\" + 0.054*\"year\" + 0.035*\"high\" + 0.033*\"college\" + 0.027*\"parent\" + 0.021*\"grade\" + 0.020*\"class\" + 0.017*\"summer\" + 0.015*\"study\" + 0.015*\"time\"'), (8, '0.152*\"sleep\" + 0.112*\"night\" + 0.091*\"pill\" + 0.061*\"morning\" + 0.055*\"bed\" + 0.023*\"shower\" + 0.022*\"usual\" + 0.022*\"nervous\" + 0.020*\"awake\" + 0.019*\"sleeping\"'), (18, '0.325*\"friend\" + 0.050*\"talk\" + 0.046*\"lonely\" + 0.046*\"good\" + 0.042*\"family\" + 0.037*\"close\" + 0.027*\"group\" + 0.023*\"alone\" + 0.021*\"new\" + 0.018*\"girl\"'), (5, '0.271*\"job\" + 0.175*\"work\" + 0.158*\"money\" + 0.031*\"food\" + 0.026*\"cat\" + 0.023*\"home\" + 0.021*\"tooth\" + 0.016*\"new\" + 0.014*\"homeless\" + 0.014*\"trash\"'), (23, '0.255*\"thought\" + 0.243*\"suicidal\" + 0.086*\"self\" + 0.042*\"harm\" + 0.031*\"amp\" + 0.022*\"wrist\" + 0.019*\"intrusive\" + 0.016*\"bridge\" + 0.015*\"helpful\" + 0.014*\"plan\"'), (27, '0.077*\"car\" + 0.051*\"husband\" + 0.040*\"accident\" + 0.037*\"chronic\" + 0.035*\"damage\" + 0.034*\"baby\" + 0.029*\"crisis\" + 0.026*\"asshole\" + 0.021*\"knife\" + 0.021*\"neck\"'), (20, '0.213*\"people\" + 0.036*\"world\" + 0.027*\"game\" + 0.024*\"social\" + 0.022*\"video\" + 0.020*\"many\" + 0.019*\"person\" + 0.018*\"other\" + 0.017*\"human\" + 0.014*\"society\"'), (1, '0.205*\"suicide\" + 0.143*\"help\" + 0.065*\"therapy\" + 0.063*\"therapist\" + 0.039*\"attempt\" + 0.037*\"note\" + 0.026*\"support\" + 0.022*\"talk\" + 0.022*\"session\" + 0.017*\"advice\"'), (11, '0.110*\"love\" + 0.054*\"sorry\" + 0.049*\"heart\" + 0.044*\"boyfriend\" + 0.036*\"message\" + 0.033*\"text\" + 0.032*\"word\" + 0.032*\"joke\" + 0.023*\"sexual\" + 0.021*\"funny\"'), (28, '0.028*\"life\" + 0.027*\"thing\" + 0.024*\"time\" + 0.018*\"thought\" + 0.018*\"mind\" + 0.015*\"way\" + 0.014*\"head\" + 0.013*\"brain\" + 0.012*\"pain\" + 0.012*\"world\"'), (4, '0.061*\"body\" + 0.051*\"ugly\" + 0.049*\"loneliness\" + 0.047*\"self\" + 0.044*\"weight\" + 0.039*\"hair\" + 0.032*\"fat\" + 0.028*\"low\" + 0.024*\"face\" + 0.023*\"mirror\"'), (15, '0.057*\"day\" + 0.053*\"time\" + 0.033*\"month\" + 0.032*\"year\" + 0.030*\"week\" + 0.023*\"last\" + 0.020*\"bad\" + 0.020*\"thing\" + 0.018*\"hour\" + 0.017*\"work\"')]\n"
     ]
    }
   ],
   "source": [
    "if doc_term_matrix:\n",
    "    LDA = gensim.models.ldamodel.LdaModel\n",
    "    lda_model = LDA(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=dictionary,\n",
    "        num_topics=30,\n",
    "        random_state=100,\n",
    "        chunksize=1000,\n",
    "        passes=50,\n",
    "        iterations=100\n",
    "    )\n",
    "    print(lda_model.print_topics())\n",
    "else:\n",
    "    print(\"Document term matrix is empty, cannot build LDA model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f59615-3803-4863-98da-a2448c3bf659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: -10.119497604980612\n",
      "Coherence: 0.39603295433127383\n"
     ]
    }
   ],
   "source": [
    "total_docs = len(doc_term_matrix)\n",
    "if total_docs > 0:\n",
    "    print('\\nPerplexity:', lda_model.log_perplexity(\n",
    "        doc_term_matrix, total_docs=total_docs))\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=tokenized_text,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence:', coherence_lda)\n",
    "else:\n",
    "    print(\"No documents to evaluate coherence or perplexity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4234694c-06e3-4f8b-9c0c-ec340f130fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if total_docs > 0:\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis_data = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "    vis_data\n",
    "    pyLDAvis.save_html(vis_data, '30_reddit_lda_visualization.html')\n",
    "else:\n",
    "    print(\"No documents for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4444c-fe53-4894-a610-6701eb6e33bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
